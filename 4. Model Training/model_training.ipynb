{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84b8778d",
   "metadata": {},
   "source": [
    "# Model Training for Insurance Fraud Detection\n",
    "\n",
    "This notebook focuses on developing predictive models to detect fraudulent insurance claims. It begins by loading the preprocessed and feature engineered dataset, followed by splitting the data into training and testing subsets to enable unbiased evaluation.\n",
    "\n",
    "We then train multiple classification models including Logistic Regression as a baseline and Random Forest as a more powerful, nonlinear model and evaluate their performance with metrics suited for imbalanced classification problems, such as precision, recall, F1-score, ROC AUC, and Precision Recall AUC.\n",
    "\n",
    "The notebook concludes with model comparison, visualization of key results, feature importance analysis for interpretability, and saving the best performing model pipeline for deployment and integration into reporting tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506bb026",
   "metadata": {},
   "source": [
    "### Notebook Setup & Imports\n",
    "\n",
    "This initial step sets up the environment by importing all necessary libraries and tools required for data manipulation, visualization, machine learning model training, evaluation, and saving. I also configured the environment to suppress warnings for cleaner output and establish consistent plot styling. This foundational setup ensures reproducibility and standardizes the coding environment before I begin any data operations or modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a25ea5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, f1_score, \n",
    "    roc_auc_score, average_precision_score, precision_recall_curve, roc_curve\n",
    ")\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "# Ignore warnings for clean notebook output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plot style for consistency\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "print(\"Libraries loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd3b81b",
   "metadata": {},
   "source": [
    "### 1. Load Engineered Features\n",
    "\n",
    "We load the fully preprocessed and feature engineered dataset prepared in earlier ETL and feature engineering steps. This dataset includes cleaned data with all necessary transformations and encoded variables, making it ready for model training. Confirming the data shape and previewing initial records help verify successful loading and give a quick snapshot of the dataset structure.\n",
    "\n",
    "Purpose:\n",
    "- Load the cleaned and feature engineered dataset\n",
    "- Confirm dataset shape and preview first rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07677bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define project directory and dataset file path\n",
    "project_dir = r\"C:\\\\Users\\\\Cloud\\\\OneDrive\\\\Desktop\\\\Fraud_Analytics_Project\"\n",
    "feature_file = os.path.join(project_dir, \"data\", \"features\", \"engineered_insurance_claims.csv\")\n",
    "\n",
    "# Load dataset into a DataFrame\n",
    "df = pd.read_csv(feature_file)\n",
    "\n",
    "print(\"Feature dataset loaded. Shape:\", df.shape)\n",
    "\n",
    "# Preview first five rows to verify data loaded correctly\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfc18e8",
   "metadata": {},
   "source": [
    "### 2. Define Features and Target\n",
    "\n",
    "Here, I separate the dataset into explanatory variables (`X`) and the target variable (`y`), which indicates whether a claim was fraudulent. I standardize all numeric features to normalize their scales. This is critical for models sensitive to feature magnitude differences, such as Logistic Regression, ensuring faster convergence and improved performance.\n",
    "\n",
    "Purpose:\n",
    "- Separate features `(X)` from target `(y)`\n",
    "- Standardize numeric features for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a67c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop target column to get features\n",
    "X = df.drop(columns=['fraud_reported'])\n",
    "\n",
    "# Target variable\n",
    "y = df['fraud_reported']\n",
    "\n",
    "# Save feature column names for later use\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "# Initialize standard scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit scaler and transform features, keep DataFrame format for clarity\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=feature_names)\n",
    "\n",
    "print(\"Features and target separated. Scaling complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a326792a",
   "metadata": {},
   "source": [
    "### 3. Train-Test Split\n",
    "\n",
    "To fairly evaluate model generalization, I split the dataset into a training set used for model learning and a test set reserved for unbiased performance evaluation. Stratification maintains the original class imbalance distribution in both subsets, which is important due to the rarity of fraud cases and helps prevent biased model assessment.\n",
    "\n",
    "Purpose:\n",
    "- Split dataset into training and testing subsets\n",
    "- Stratify to maintain fraud class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011245cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform train-test split (75% train, 25% test), stratify on target to balance classes\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train-test split complete.\")\n",
    "print(\"Training set:\", X_train.shape)\n",
    "print(\"Test set    :\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5500f865",
   "metadata": {},
   "source": [
    "### 4. Baseline Model - Logistic Regression\n",
    "\n",
    "We implement Logistic Regression as a baseline model due to its interpretability and efficiency on tabular data. This simple linear model provides a benchmark performance level and insight into feature influence via coefficients. The classification report summarizes key metrics (precision, recall, F1-score), guiding us on model strengths and weaknesses in fraud detection.\n",
    "\n",
    "Purpose:\n",
    "- Train logistic regression as baseline classifier\n",
    "- Generate classification report for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac9f22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Logistic Regression model with increased max iterations for convergence\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Fit model on training data\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_lr = logreg.predict(X_test)\n",
    "\n",
    "# Display detailed classification metrics\n",
    "print(\"Logistic Regression Report\")\n",
    "print(classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7660790b",
   "metadata": {},
   "source": [
    "### 5. Train Random Forest Classifier\n",
    "\n",
    "Next, I train a Random Forest model, this method builds multiple decision trees and aggregates their predictions. Random Forest often excels with tabular datasets and can capture complex nonlinear relationships. Comparing its performance to Logistic Regression helps me identify a more accurate and robust fraud detection model.\n",
    "\n",
    "Purpose:\n",
    "- Train a Random Forest, a robust tree-based classifier\n",
    "- Compare performance with baseline Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc75855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Random Forest with 100 trees\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit model on training data\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Show classification metrics for Random Forest\n",
    "print(\"Random Forest Report\")\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ad57e8",
   "metadata": {},
   "source": [
    "### 6. Visualize Confusion Matrices\n",
    "\n",
    "Confusion matrices visually depict classification outcomes, showing true positives, false positives, true negatives, and false negatives. This step will help stakeholders understand types of errors each model makes, crucial in fraud detection where minimizing false negatives (missed frauds) can have significant cost implications.\n",
    "\n",
    "Purpose:\n",
    "- Plot confusion matrices to visually assess classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78141964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_conf_matrix(y_true, y_pred, title):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix with annotations.\n",
    "    \n",
    "    Parameters:\n",
    "    y_true: array-like, ground truth labels\n",
    "    y_pred: array-like, predicted labels\n",
    "    title: str, plot title\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.show()\n",
    "\n",
    "# Plot for Logistic Regression\n",
    "plot_conf_matrix(y_test, y_pred_lr, \"Logistic Regression Confusion Matrix\")\n",
    "\n",
    "# Plot for Random Forest\n",
    "plot_conf_matrix(y_test, y_pred_rf, \"Random Forest Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d88bf36",
   "metadata": {},
   "source": [
    "### 7. Performance Comparison\n",
    "\n",
    "I quantitatively compare the models using the F1-score, a harmonic mean of precision and recall, which is a balanced measure especially valuable in imbalanced datasets like fraud detection. This informs which model better balances identifying fraud cases and limiting false alarms.\n",
    "\n",
    "Purpose:\n",
    "- Compare F1 scores of Logistic Regression and Random Forest models\n",
    "- Use F1 due to class imbalance in fraud detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba9c2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F1 scores for both models\n",
    "f1_lr = f1_score(y_test, y_pred_lr)\n",
    "f1_rf = f1_score(y_test, y_pred_rf)\n",
    "\n",
    "print(f\"F1 Score - Logistic Regression: {f1_lr:.4f}\")\n",
    "print(f\"F1 Score - Random Forest      : {f1_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398aa08b",
   "metadata": {},
   "source": [
    "### 8. Evaluate Probabilistic Metrics\n",
    "\n",
    "Beyond binary predictions, I assess model performance using probabilistic metrics: ROC AUC (Receiver Operating Characteristic Area Under Curve) and Precision Recall AUC. These metrics provide a nuanced view of model discrimination capability and effectiveness at various classification thresholds, critical in real world fraud risk prioritization.\n",
    "\n",
    "Purpose:\n",
    "- Compute ROC AUC and Precision Recall AUC for Random Forest\n",
    "- Useful metrics under class imbalanc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d90c013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict fraud probabilities for positive class\n",
    "y_prob_rf = rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate ROC AUC (area under ROC curve)\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob_rf))\n",
    "\n",
    "# Calculate Precision-Recall AUC\n",
    "print(\"PR AUC :\", average_precision_score(y_test, y_prob_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a36dbe",
   "metadata": {},
   "source": [
    "### 9. ROC and Precision Recall Curves\n",
    "\n",
    "Visualizing ROC and Precision Recall curves enables stakeholders to see how the modelâ€™s performance varies with different decision thresholds. These plots help choose thresholds that balance catching frauds and avoiding false positives, aligning model behavior with business risk appetite.\n",
    "\n",
    "Purpose:\n",
    "- Plot ROC and Precision Recall curves to visualize model threshold effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce6373b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob_rf)\n",
    "\n",
    "# Compute Precision-Recall curve\n",
    "prec, recall, _ = precision_recall_curve(y_test, y_prob_rf)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# ROC curve subplot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(fpr, tpr, label=\"ROC Curve\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "\n",
    "# Precision-Recall curve subplot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(recall, prec, label=\"PR Curve\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eaee470",
   "metadata": {},
   "source": [
    "### 10. Feature Importances (Random Forest)\n",
    "\n",
    "Here, I extract and visualize the most influential features driving the Random Forest modelâ€™s decisions. Understanding which variables contribute most to detecting fraud aids in model explainability and builds stakeholder trust. It can also guide future data collection and feature engineering efforts.\n",
    "\n",
    "Purpose:\n",
    "- Identify the most important features used by the Random Forest model\n",
    "- Helps with interpretability and insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d0ce5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importances\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "# Sort and take top 20 features\n",
    "feat_imp = pd.Series(importances, index=feature_names).sort_values(ascending=False).head(20)\n",
    "\n",
    "# Plot feature importances as horizontal bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=feat_imp.values, y=feat_imp.index)\n",
    "plt.title(\"Top 20 Feature Importances\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e23c422",
   "metadata": {},
   "source": [
    "### 11. Save Best Model as Pipeline\n",
    "\n",
    "For consistent deployment and future scoring, I save the entire processing and modeling pipeline (scaling + Random Forest) as a single object. This ensures that new data undergoes the same preprocessing steps before prediction, reducing errors and streamlining integration into production or dashboards.\n",
    "\n",
    "Purpose:\n",
    "- Save entire pipeline with scaler and Random Forest\n",
    "- Ensures consistent preprocessing in deployment or dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb8f1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline that applies scaling and Random Forest in sequence\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Fit pipeline on training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Create model directory if not exists\n",
    "model_dir = os.path.join(project_dir, \"models\")\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save pipeline to disk\n",
    "model_path = os.path.join(model_dir, \"fraud_model_pipeline_rf.joblib\")\n",
    "joblib.dump(pipeline, model_path)\n",
    "\n",
    "print(f\"Best model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dd1f16",
   "metadata": {},
   "source": [
    "### 12. Metadata Logging\n",
    "\n",
    "I log library versions and run timestamps for reproducibility and audit purposes. This practice helps track the software environment under which the model was trained, supporting future debugging, retraining, and regulatory compliance.\n",
    "\n",
    "Purpose:\n",
    "- Log package versions and run date for reproducibility and reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b173466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import datetime\n",
    "\n",
    "print(\"ðŸ”§ Versions and Metadata\")\n",
    "print(\"sklearn version:\", sklearn.__version__)\n",
    "print(\"pandas version:\", pd.__version__)\n",
    "print(\"Run date       :\", datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd61aaff",
   "metadata": {},
   "source": [
    "### 13. Next Steps\n",
    "\n",
    "This outlines the immediate actions following model training: integrating the saved model into the reporting dashboard, generating fraud risk predictions on new claims, and preparing for deployment. These steps close the analytics loop and deliver actionable insights to stakeholders.\n",
    "\n",
    "- Load this model pipeline in `reporting_dashboard.ipynb`\n",
    "- Generate fraud prediction outputs\n",
    "- Deploy model for live or batch scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864339cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Next Steps:\")\n",
    "print(\"- Use `fraud_model_pipeline_rf.joblib` in dashboard notebook\")\n",
    "print(\"- Visualize predictions and high-risk fraud cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b7abf2",
   "metadata": {},
   "source": [
    "# Conclusion and Next Steps\n",
    "\n",
    "This notebook successfully developed and evaluated machine learning models for insurance fraud detection. The Random Forest classifier outperformed the baseline Logistic Regression model, providing higher predictive accuracy and robustness on this imbalanced dataset.\n",
    "\n",
    "By saving the full modeling pipeline, including preprocessing, I ensure reproducible and consistent predictions during deployment.\n",
    "\n",
    "Next steps include integrating this model into the reporting dashboard to generate real time fraud risk scores, enabling business stakeholders to prioritize investigations and reduce fraud related losses. Further enhancements could involve hyperparameter tuning, incorporating additional data sources, or exploring alternative algorithms to continually improve fraud detection effectiveness."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
