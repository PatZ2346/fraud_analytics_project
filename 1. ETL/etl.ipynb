{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89ed362b",
   "metadata": {},
   "source": [
    "# ETL Pipeline for Insurance Fraud Detection\n",
    "\n",
    "This notebook implements a structured **Extract, Transform, Load (ETL)** pipeline on raw insurance claims data I found on kaggle, link here: [Auto Insurance Claims Fraud dataset](https://www.kaggle.com/datasets/antopravinjohnbosco/auto-insurance-claims-fraud-detection?resource=download). The goal is to clean and prepare the dataset for accurate fraud detection analysis and subsequently load the processed data into a **PostgreSQL** database for efficient querying.\n",
    "\n",
    "### Pipeline Overview:\n",
    "- Load raw insurance claims data from CSV\n",
    "- Conduct thorough data integrity and quality checks\n",
    "- Handle missing and inconsistent data with appropriate transformations\n",
    "- Engineer key features to support fraud detection modeling\n",
    "- Save the cleaned dataset as a reusable **CSV**\n",
    "- Load cleaned data into a PostgreSQL table (`insurance_claims`)\n",
    "- Run SQL queries to validate data loading and summarize fraud occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40960bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import & load essential libraries\n",
    "import pandas as pd                       # For data manipulation and cleaning\n",
    "import os                                 # For managing file paths\n",
    "from sqlalchemy import create_engine      # For database connection to PostgreSQL\n",
    "from dotenv import load_dotenv            # For loading environment variables from .env file\n",
    "load_dotenv()                             # Load environment variables from .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2279c88",
   "metadata": {},
   "source": [
    "## 1. Define File Paths & PostgreSQL Connection\n",
    "\n",
    "Set up input/output file locations and connect to my **PostgreSQL** database using **SQLAlchemy**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21623a7",
   "metadata": {},
   "source": [
    "### Define File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c5adf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base project directory\n",
    "project_dir = r\"C:\\Users\\Cloud\\OneDrive\\Desktop\\Fraud_Analytics_Project\"\n",
    "\n",
    "# Define path to the raw insurance claims CSV dataset\n",
    "dataset_path = os.path.join(project_dir, \"dataset\", \"insurance_fraud_claims.csv\")\n",
    "\n",
    "# Define directory to save cleaned data outputs\n",
    "cleaned_dir = os.path.join(project_dir, \"data\", \"cleaned\")\n",
    "os.makedirs(cleaned_dir, exist_ok=True)\n",
    "\n",
    "# Define path to save cleaned dataset\n",
    "cleaned_file = os.path.join(cleaned_dir, \"cleaned_insurance_claims.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8d1e24",
   "metadata": {},
   "source": [
    "### PostgreSQL Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d9731c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PostgreSQL credentials, mine is stored in a .env file\n",
    "pg_user = os.getenv(\"PG_USER\")              # Your PostgreSQL username\n",
    "pg_password = os.getenv(\"PG_PASSWORD\")      # Your password\n",
    "pg_host = os.getenv(\"PG_HOST\")              # Host where PostgreSQL is running (usually localhost)\n",
    "pg_port = os.getenv(\"PG_PORT\")              # Default PostgreSQL port\n",
    "pg_dbname = os.getenv(\"PG_DBNAME\")          # The database you've created in pgAdmin\n",
    "\n",
    "# Safety check to ensure no credentials are missing\n",
    "required_env_vars = [pg_user, pg_password, pg_host, pg_port, pg_dbname]\n",
    "if any(v is None for v in required_env_vars):\n",
    "    raise ValueError(\"One or more environment variables are missing. Check your .env file.\")\n",
    "\n",
    "# Establish connection engine to PostgreSQL database for data loading and querying\n",
    "engine = create_engine(\n",
    "    f\"postgresql+psycopg2://{pg_user}:{pg_password}@{pg_host}:{pg_port}/{pg_dbname}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877a7013",
   "metadata": {},
   "source": [
    "## 2. Load Raw Insurance Claims Dataset\n",
    "\n",
    "Read the original dataset into a `pandas` DataFrame and preview its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b068be5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data loaded. Shape: (1000, 40)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "months_as_customer",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "age",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "policy_number",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "policy_bind_date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "policy_state",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "policy_csl",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "policy_deductable",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "policy_annual_premium",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "umbrella_limit",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "insured_zip",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "insured_sex",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "insured_education_level",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "insured_occupation",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "insured_hobbies",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "insured_relationship",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "capital-gains",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "capital-loss",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "incident_date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "incident_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "collision_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "incident_severity",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "authorities_contacted",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "incident_state",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "incident_city",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "incident_location",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "incident_hour_of_the_day",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "number_of_vehicles_involved",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "property_damage",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "bodily_injuries",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "witnesses",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "police_report_available",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "total_claim_amount",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "injury_claim",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "property_claim",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "vehicle_claim",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "auto_make",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "auto_model",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "auto_year",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "fraud_reported",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "_c39",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "9d97dcf5-b655-4ac5-a53c-d079e9a28af8",
       "rows": [
        [
         "0",
         "328",
         "48",
         "521585",
         "2014-10-17",
         "OH",
         "250/500",
         "1000",
         "1406.91",
         "0",
         "466132",
         "MALE",
         "MD",
         "craft-repair",
         "sleeping",
         "husband",
         "53300",
         "0",
         "2015-01-25",
         "Single Vehicle Collision",
         "Side Collision",
         "Major Damage",
         "Police",
         "SC",
         "Columbus",
         "9935 4th Drive",
         "5",
         "1",
         "YES",
         "1",
         "2",
         "YES",
         "71610",
         "6510",
         "13020",
         "52080",
         "Saab",
         "92x",
         "2004",
         "Y",
         null
        ],
        [
         "1",
         "228",
         "42",
         "342868",
         "2006-06-27",
         "IN",
         "250/500",
         "2000",
         "1197.22",
         "5000000",
         "468176",
         "MALE",
         "MD",
         "machine-op-inspct",
         "reading",
         "other-relative",
         "0",
         "0",
         "2015-01-21",
         "Vehicle Theft",
         "?",
         "Minor Damage",
         "Police",
         "VA",
         "Riverwood",
         "6608 MLK Hwy",
         "8",
         "1",
         "?",
         "0",
         "0",
         "?",
         "5070",
         "780",
         "780",
         "3510",
         "Mercedes",
         "E400",
         "2007",
         "Y",
         null
        ],
        [
         "2",
         "134",
         "29",
         "687698",
         "2000-09-06",
         "OH",
         "100/300",
         "2000",
         "1413.14",
         "5000000",
         "430632",
         "FEMALE",
         "PhD",
         "sales",
         "board-games",
         "own-child",
         "35100",
         "0",
         "2015-02-22",
         "Multi-vehicle Collision",
         "Rear Collision",
         "Minor Damage",
         "Police",
         "NY",
         "Columbus",
         "7121 Francis Lane",
         "7",
         "3",
         "NO",
         "2",
         "3",
         "NO",
         "34650",
         "7700",
         "3850",
         "23100",
         "Dodge",
         "RAM",
         "2007",
         "N",
         null
        ],
        [
         "3",
         "256",
         "41",
         "227811",
         "1990-05-25",
         "IL",
         "250/500",
         "2000",
         "1415.74",
         "6000000",
         "608117",
         "FEMALE",
         "PhD",
         "armed-forces",
         "board-games",
         "unmarried",
         "48900",
         "-62400",
         "2015-01-10",
         "Single Vehicle Collision",
         "Front Collision",
         "Major Damage",
         "Police",
         "OH",
         "Arlington",
         "6956 Maple Drive",
         "5",
         "1",
         "?",
         "1",
         "2",
         "NO",
         "63400",
         "6340",
         "6340",
         "50720",
         "Chevrolet",
         "Tahoe",
         "2014",
         "Y",
         null
        ],
        [
         "4",
         "228",
         "44",
         "367455",
         "2014-06-06",
         "IL",
         "500/1000",
         "1000",
         "1583.91",
         "6000000",
         "610706",
         "MALE",
         "Associate",
         "sales",
         "board-games",
         "unmarried",
         "66000",
         "-46000",
         "2015-02-17",
         "Vehicle Theft",
         "?",
         "Minor Damage",
         null,
         "NY",
         "Arlington",
         "3041 3rd Ave",
         "20",
         "1",
         "NO",
         "0",
         "1",
         "NO",
         "6500",
         "1300",
         "650",
         "4550",
         "Accura",
         "RSX",
         "2009",
         "N",
         null
        ]
       ],
       "shape": {
        "columns": 40,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>months_as_customer</th>\n",
       "      <th>age</th>\n",
       "      <th>policy_number</th>\n",
       "      <th>policy_bind_date</th>\n",
       "      <th>policy_state</th>\n",
       "      <th>policy_csl</th>\n",
       "      <th>policy_deductable</th>\n",
       "      <th>policy_annual_premium</th>\n",
       "      <th>umbrella_limit</th>\n",
       "      <th>insured_zip</th>\n",
       "      <th>...</th>\n",
       "      <th>police_report_available</th>\n",
       "      <th>total_claim_amount</th>\n",
       "      <th>injury_claim</th>\n",
       "      <th>property_claim</th>\n",
       "      <th>vehicle_claim</th>\n",
       "      <th>auto_make</th>\n",
       "      <th>auto_model</th>\n",
       "      <th>auto_year</th>\n",
       "      <th>fraud_reported</th>\n",
       "      <th>_c39</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>328</td>\n",
       "      <td>48</td>\n",
       "      <td>521585</td>\n",
       "      <td>2014-10-17</td>\n",
       "      <td>OH</td>\n",
       "      <td>250/500</td>\n",
       "      <td>1000</td>\n",
       "      <td>1406.91</td>\n",
       "      <td>0</td>\n",
       "      <td>466132</td>\n",
       "      <td>...</td>\n",
       "      <td>YES</td>\n",
       "      <td>71610</td>\n",
       "      <td>6510</td>\n",
       "      <td>13020</td>\n",
       "      <td>52080</td>\n",
       "      <td>Saab</td>\n",
       "      <td>92x</td>\n",
       "      <td>2004</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>228</td>\n",
       "      <td>42</td>\n",
       "      <td>342868</td>\n",
       "      <td>2006-06-27</td>\n",
       "      <td>IN</td>\n",
       "      <td>250/500</td>\n",
       "      <td>2000</td>\n",
       "      <td>1197.22</td>\n",
       "      <td>5000000</td>\n",
       "      <td>468176</td>\n",
       "      <td>...</td>\n",
       "      <td>?</td>\n",
       "      <td>5070</td>\n",
       "      <td>780</td>\n",
       "      <td>780</td>\n",
       "      <td>3510</td>\n",
       "      <td>Mercedes</td>\n",
       "      <td>E400</td>\n",
       "      <td>2007</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>134</td>\n",
       "      <td>29</td>\n",
       "      <td>687698</td>\n",
       "      <td>2000-09-06</td>\n",
       "      <td>OH</td>\n",
       "      <td>100/300</td>\n",
       "      <td>2000</td>\n",
       "      <td>1413.14</td>\n",
       "      <td>5000000</td>\n",
       "      <td>430632</td>\n",
       "      <td>...</td>\n",
       "      <td>NO</td>\n",
       "      <td>34650</td>\n",
       "      <td>7700</td>\n",
       "      <td>3850</td>\n",
       "      <td>23100</td>\n",
       "      <td>Dodge</td>\n",
       "      <td>RAM</td>\n",
       "      <td>2007</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>256</td>\n",
       "      <td>41</td>\n",
       "      <td>227811</td>\n",
       "      <td>1990-05-25</td>\n",
       "      <td>IL</td>\n",
       "      <td>250/500</td>\n",
       "      <td>2000</td>\n",
       "      <td>1415.74</td>\n",
       "      <td>6000000</td>\n",
       "      <td>608117</td>\n",
       "      <td>...</td>\n",
       "      <td>NO</td>\n",
       "      <td>63400</td>\n",
       "      <td>6340</td>\n",
       "      <td>6340</td>\n",
       "      <td>50720</td>\n",
       "      <td>Chevrolet</td>\n",
       "      <td>Tahoe</td>\n",
       "      <td>2014</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>228</td>\n",
       "      <td>44</td>\n",
       "      <td>367455</td>\n",
       "      <td>2014-06-06</td>\n",
       "      <td>IL</td>\n",
       "      <td>500/1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1583.91</td>\n",
       "      <td>6000000</td>\n",
       "      <td>610706</td>\n",
       "      <td>...</td>\n",
       "      <td>NO</td>\n",
       "      <td>6500</td>\n",
       "      <td>1300</td>\n",
       "      <td>650</td>\n",
       "      <td>4550</td>\n",
       "      <td>Accura</td>\n",
       "      <td>RSX</td>\n",
       "      <td>2009</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   months_as_customer  age  policy_number policy_bind_date policy_state  \\\n",
       "0                 328   48         521585       2014-10-17           OH   \n",
       "1                 228   42         342868       2006-06-27           IN   \n",
       "2                 134   29         687698       2000-09-06           OH   \n",
       "3                 256   41         227811       1990-05-25           IL   \n",
       "4                 228   44         367455       2014-06-06           IL   \n",
       "\n",
       "  policy_csl  policy_deductable  policy_annual_premium  umbrella_limit  \\\n",
       "0    250/500               1000                1406.91               0   \n",
       "1    250/500               2000                1197.22         5000000   \n",
       "2    100/300               2000                1413.14         5000000   \n",
       "3    250/500               2000                1415.74         6000000   \n",
       "4   500/1000               1000                1583.91         6000000   \n",
       "\n",
       "   insured_zip  ... police_report_available total_claim_amount injury_claim  \\\n",
       "0       466132  ...                     YES              71610         6510   \n",
       "1       468176  ...                       ?               5070          780   \n",
       "2       430632  ...                      NO              34650         7700   \n",
       "3       608117  ...                      NO              63400         6340   \n",
       "4       610706  ...                      NO               6500         1300   \n",
       "\n",
       "  property_claim vehicle_claim  auto_make  auto_model auto_year  \\\n",
       "0          13020         52080       Saab         92x      2004   \n",
       "1            780          3510   Mercedes        E400      2007   \n",
       "2           3850         23100      Dodge         RAM      2007   \n",
       "3           6340         50720  Chevrolet       Tahoe      2014   \n",
       "4            650          4550     Accura         RSX      2009   \n",
       "\n",
       "  fraud_reported _c39  \n",
       "0              Y  NaN  \n",
       "1              Y  NaN  \n",
       "2              N  NaN  \n",
       "3              Y  NaN  \n",
       "4              N  NaN  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the raw insurance claims data into a pandas DataFrame\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Preview and display first few rows of DataFrame structure\n",
    "print(\"Raw data loaded. Shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7968a70",
   "metadata": {},
   "source": [
    "**Quick exploratory check on distribution for missing or unusual values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2571b6ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "incident_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "c83338f7-1b42-474e-a5bb-2cbcf282f68b",
       "rows": [
        [
         "Multi-vehicle Collision",
         "419"
        ],
        [
         "Single Vehicle Collision",
         "403"
        ],
        [
         "Vehicle Theft",
         "94"
        ],
        [
         "Parked Car",
         "84"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 4
       }
      },
      "text/plain": [
       "incident_type\n",
       "Multi-vehicle Collision     419\n",
       "Single Vehicle Collision    403\n",
       "Vehicle Theft                94\n",
       "Parked Car                   84\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check distribution of 'collision_type', including missing values\n",
    "df['collision_type'].value_counts(dropna=False)\n",
    "\n",
    "# Check distribution of 'incident_type', including missing values\n",
    "df['incident_type'].value_counts(dropna=False)\n",
    "\n",
    "# Check distribution of 'collision_type' excluding missing values (default behavior)\n",
    "df['collision_type'].value_counts()\n",
    "\n",
    "# Check distribution of 'incident_type' excluding missing values\n",
    "df['incident_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c665ec",
   "metadata": {},
   "source": [
    "## 3. Replace Placeholder Values with Standard Missing Marker\n",
    "\n",
    "Many datasets use placeholders like `\"?\"`, `\"unknown\"`, `\"n/a\"`, or `\"missing\"` to indicate missing or unclear data.  \n",
    "To ensure consistent and accurate analysis, I replaced these placeholders with `pandas` standard missing value marker (`pd.NA`).  \n",
    "\n",
    "This helps downstream processes correctly recognize and handle missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec792585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Placeholder values like '?' replaced with pd.NA.\n",
      "\n",
      "Normalized 'fraud_reported' values to uppercase and stripped whitespace.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List of placeholder values commonly used for missing data\n",
    "placeholders = ['?', 'unknown', 'n/a', 'missing']\n",
    "\n",
    "# Replace all such placeholders with pandas standard missing value marker pd.NA\n",
    "df.replace(placeholders, pd.NA, inplace=True)\n",
    "print(\"Placeholder values like '?' replaced with pd.NA.\\n\")\n",
    "\n",
    "# Normalize 'fraud_reported' for consistent validation and mapping\n",
    "if 'fraud_reported' in df.columns:\n",
    "    df['fraud_reported'] = df['fraud_reported'].astype(str).str.strip().str.upper()\n",
    "    print(\"Normalized 'fraud_reported' values to uppercase and stripped whitespace.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b378fca",
   "metadata": {},
   "source": [
    "## 4. Normalize All String Columns For Uniformity\n",
    "\n",
    "Here I need to standardize string based fields to ensure consistency across the dataset. \n",
    "String values (like `'yes'`, `'Yes'`, `' YES '`) may represent the same category but appear different to a computer. This step ensures all string columns are uniformly formatted for reliable processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "192bcb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized all string columns: stripped whitespace and converted text to uppercase.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Normalize all string and object columns to ensure consistent formatting\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "    df[col] = df[col].astype(str).str.strip().str.upper()\n",
    "    \n",
    "print(\"Normalized all string columns: stripped whitespace and converted text to uppercase.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7990ae",
   "metadata": {},
   "source": [
    "## 5. Data Integrity and Validity Checks\n",
    "\n",
    "In this step, I have performed essential checks to ensure the dataset's quality and reliability before analysis, including:\n",
    "\n",
    "- Confirming the overall shape and structure of the dataset  \n",
    "- Identifying any duplicate records  \n",
    "- Summarizing missing values and their prevalence  \n",
    "- Verifying data types for each column  \n",
    "- Reviewing unique values in categorical columns to detect anomalies or typos  \n",
    "- Validating that key columns, such as `fraud_reported`, contain only expected values `\"Y\" or \"N\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e3bfbec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Integrity Checks\n",
      "Dataset shape: (1000, 40)\n",
      "\n",
      "Column Names and Data Types:\n",
      "months_as_customer               int64\n",
      "age                              int64\n",
      "policy_number                    int64\n",
      "policy_bind_date                object\n",
      "policy_state                    object\n",
      "policy_csl                      object\n",
      "policy_deductable                int64\n",
      "policy_annual_premium          float64\n",
      "umbrella_limit                   int64\n",
      "insured_zip                      int64\n",
      "insured_sex                     object\n",
      "insured_education_level         object\n",
      "insured_occupation              object\n",
      "insured_hobbies                 object\n",
      "insured_relationship            object\n",
      "capital-gains                    int64\n",
      "capital-loss                     int64\n",
      "incident_date                   object\n",
      "incident_type                   object\n",
      "collision_type                  object\n",
      "incident_severity               object\n",
      "authorities_contacted           object\n",
      "incident_state                  object\n",
      "incident_city                   object\n",
      "incident_location               object\n",
      "incident_hour_of_the_day         int64\n",
      "number_of_vehicles_involved      int64\n",
      "property_damage                 object\n",
      "bodily_injuries                  int64\n",
      "witnesses                        int64\n",
      "police_report_available         object\n",
      "total_claim_amount               int64\n",
      "injury_claim                     int64\n",
      "property_claim                   int64\n",
      "vehicle_claim                    int64\n",
      "auto_make                       object\n",
      "auto_model                      object\n",
      "auto_year                        int64\n",
      "fraud_reported                  object\n",
      "_c39                           float64\n",
      "dtype: object\n",
      "\n",
      "Duplicate rows: 0\n",
      "\n",
      "Missing Values:\n",
      "      Missing Count  Percent\n",
      "_c39           1000    100.0\n",
      "\n",
      "Unique values in categorical columns:\n",
      "policy_bind_date: 951 unique values\n",
      "  → ['2014-10-17' '2006-06-27' '2000-09-06' '1990-05-25' '2014-06-06'] ...\n",
      "policy_state: 3 unique values\n",
      "  → ['OH' 'IN' 'IL'] ...\n",
      "policy_csl: 3 unique values\n",
      "  → ['250/500' '100/300' '500/1000'] ...\n",
      "insured_sex: 2 unique values\n",
      "  → ['MALE' 'FEMALE'] ...\n",
      "insured_education_level: 7 unique values\n",
      "  → ['MD' 'PHD' 'ASSOCIATE' 'MASTERS' 'HIGH SCHOOL'] ...\n",
      "insured_occupation: 14 unique values\n",
      "  → ['CRAFT-REPAIR' 'MACHINE-OP-INSPCT' 'SALES' 'ARMED-FORCES' 'TECH-SUPPORT'] ...\n",
      "insured_hobbies: 20 unique values\n",
      "  → ['SLEEPING' 'READING' 'BOARD-GAMES' 'BUNGIE-JUMPING' 'BASE-JUMPING'] ...\n",
      "insured_relationship: 6 unique values\n",
      "  → ['HUSBAND' 'OTHER-RELATIVE' 'OWN-CHILD' 'UNMARRIED' 'WIFE'] ...\n",
      "incident_date: 60 unique values\n",
      "  → ['2015-01-25' '2015-01-21' '2015-02-22' '2015-01-10' '2015-02-17'] ...\n",
      "incident_type: 4 unique values\n",
      "  → ['SINGLE VEHICLE COLLISION' 'VEHICLE THEFT' 'MULTI-VEHICLE COLLISION'\n",
      " 'PARKED CAR'] ...\n",
      "collision_type: 4 unique values\n",
      "  → ['SIDE COLLISION' '<NA>' 'REAR COLLISION' 'FRONT COLLISION'] ...\n",
      "incident_severity: 4 unique values\n",
      "  → ['MAJOR DAMAGE' 'MINOR DAMAGE' 'TOTAL LOSS' 'TRIVIAL DAMAGE'] ...\n",
      "authorities_contacted: 5 unique values\n",
      "  → ['POLICE' 'NAN' 'FIRE' 'OTHER' 'AMBULANCE'] ...\n",
      "incident_state: 7 unique values\n",
      "  → ['SC' 'VA' 'NY' 'OH' 'WV'] ...\n",
      "incident_city: 7 unique values\n",
      "  → ['COLUMBUS' 'RIVERWOOD' 'ARLINGTON' 'SPRINGFIELD' 'HILLSDALE'] ...\n",
      "incident_location: 1000 unique values\n",
      "  → ['9935 4TH DRIVE' '6608 MLK HWY' '7121 FRANCIS LANE' '6956 MAPLE DRIVE'\n",
      " '3041 3RD AVE'] ...\n",
      "property_damage: 3 unique values\n",
      "  → ['YES' '<NA>' 'NO'] ...\n",
      "police_report_available: 3 unique values\n",
      "  → ['YES' '<NA>' 'NO'] ...\n",
      "auto_make: 14 unique values\n",
      "  → ['SAAB' 'MERCEDES' 'DODGE' 'CHEVROLET' 'ACCURA'] ...\n",
      "auto_model: 39 unique values\n",
      "  → ['92X' 'E400' 'RAM' 'TAHOE' 'RSX'] ...\n",
      "fraud_reported: 2 unique values\n",
      "  → ['Y' 'N'] ...\n",
      "\n",
      "All expected columns are present.\n"
     ]
    }
   ],
   "source": [
    "# Data Integrity Check\n",
    "print(\"Initial Integrity Checks\")\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "\n",
    "# 1. Column names and data types\n",
    "print(\"\\nColumn Names and Data Types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# 2. Check for duplicate rows\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"\\nDuplicate rows: {duplicates}\")\n",
    "\n",
    "# 3. Check for missing values\n",
    "missing = df.isnull().sum()\n",
    "missing_percent = (missing / len(df) * 100).round(2)\n",
    "print(\"\\nMissing Values:\")\n",
    "print(pd.DataFrame({'Missing Count': missing, 'Percent': missing_percent}).loc[missing > 0])\n",
    "\n",
    "# 4. Unique values in categorical columns\n",
    "cat_cols = df.select_dtypes(include='object').columns.tolist()\n",
    "print(\"\\nUnique values in categorical columns:\")\n",
    "for col in cat_cols:\n",
    "    print(f\"{col}: {df[col].nunique()} unique values\")\n",
    "    print(f\"  → {df[col].unique()[:5]} ...\")  # Display sample of unique values to detect any irregular categories\n",
    "\n",
    "# 5. Check for expected column presence\n",
    "expected_cols = ['fraud_reported', 'incident_type', 'total_claim_amount']\n",
    "missing_cols = [col for col in expected_cols if col not in df.columns]\n",
    "if missing_cols:\n",
    "    print(f\"\\nMissing expected columns: {missing_cols}\")\n",
    "else:\n",
    "    print(\"\\nAll expected columns are present.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1daae65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'fraud_reported' column contains only 'Y' and 'N'\n"
     ]
    }
   ],
   "source": [
    "# Validate that 'fraud_reported' contains only expected values ('Y' or 'N') to prevent downstream errors\n",
    "if 'fraud_reported' in df.columns:\n",
    "    valid_fraud_values = set(df['fraud_reported'].dropna().unique())\n",
    "    if not valid_fraud_values.issubset({'Y', 'N'}):\n",
    "        print(f\"Invalid values found in 'fraud_reported': {valid_fraud_values}\")\n",
    "    else:\n",
    "        print(\"'fraud_reported' column contains only 'Y' and 'N'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c686cb32",
   "metadata": {},
   "source": [
    "## 6. Explore and Inspect Missing Data\n",
    "\n",
    "Before I clean the dataset, I need to identify which columns contain missing values and how many.  \n",
    "This helps me decide whether to **fill**, **drop**, or **investigate further** depending on the importance of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37553cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with missing values:\n",
      "      Missing Count  Missing %\n",
      "_c39           1000      100.0\n"
     ]
    }
   ],
   "source": [
    "# Count missing values for each column\n",
    "missing = df.isnull().sum()\n",
    "\n",
    "# Filter to show only columns that have missing values, sorted from most to least\n",
    "missing = missing[missing > 0].sort_values(ascending=False)\n",
    "\n",
    "# Create a DataFrame summarizing both the count and percentage of missing values\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing,\n",
    "    'Missing %': (missing / len(df) * 100).round(2)\n",
    "})\n",
    "\n",
    "# Display the summary table of missing data\n",
    "print(\"Columns with missing values:\")\n",
    "print(missing_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bd453b",
   "metadata": {},
   "source": [
    "## 7. Detect and Clean Leading/Trailing Spaces in String Columns\n",
    "\n",
    "String data can sometimes contain unwanted leading or trailing spaces that cause inconsistencies in analysis and modeling.  \n",
    "This step performs the following actions:\n",
    "\n",
    "1. **Detect** any entries in string columns that have leading or trailing whitespace and display examples to help verify the extent of the issue.\n",
    "2. **Clean** all string columns by stripping these spaces to ensure uniformity.\n",
    "3. **Verify** the cleanup by displaying sample cleaned values for confirmation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa6350a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for leading/trailing spaces in string columns...\n",
      "\n",
      "Column 'policy_bind_date' is clean (no leading/trailing spaces).\n",
      "Column 'policy_state' is clean (no leading/trailing spaces).\n",
      "Column 'policy_csl' is clean (no leading/trailing spaces).\n",
      "Column 'insured_sex' is clean (no leading/trailing spaces).\n",
      "Column 'insured_education_level' is clean (no leading/trailing spaces).\n",
      "Column 'insured_occupation' is clean (no leading/trailing spaces).\n",
      "Column 'insured_hobbies' is clean (no leading/trailing spaces).\n",
      "Column 'insured_relationship' is clean (no leading/trailing spaces).\n",
      "Column 'incident_date' is clean (no leading/trailing spaces).\n",
      "Column 'incident_type' is clean (no leading/trailing spaces).\n",
      "Column 'collision_type' is clean (no leading/trailing spaces).\n",
      "Column 'incident_severity' is clean (no leading/trailing spaces).\n",
      "Column 'authorities_contacted' is clean (no leading/trailing spaces).\n",
      "Column 'incident_state' is clean (no leading/trailing spaces).\n",
      "Column 'incident_city' is clean (no leading/trailing spaces).\n",
      "Column 'incident_location' is clean (no leading/trailing spaces).\n",
      "Column 'property_damage' is clean (no leading/trailing spaces).\n",
      "Column 'police_report_available' is clean (no leading/trailing spaces).\n",
      "Column 'auto_make' is clean (no leading/trailing spaces).\n",
      "Column 'auto_model' is clean (no leading/trailing spaces).\n",
      "Column 'fraud_reported' is clean (no leading/trailing spaces).\n",
      "\n",
      "Cleaning all string columns by stripping leading and trailing spaces...\n",
      "\n",
      "Cleanup complete. Sample cleaned values for each string column:\n",
      "policy_bind_date: ['2014-10-17' '2006-06-27' '2000-09-06' '1990-05-25' '2014-06-06']\n",
      "policy_state: ['OH' 'IN' 'IL']\n",
      "policy_csl: ['250/500' '100/300' '500/1000']\n",
      "insured_sex: ['MALE' 'FEMALE']\n",
      "insured_education_level: ['MD' 'PHD' 'ASSOCIATE' 'MASTERS' 'HIGH SCHOOL']\n",
      "insured_occupation: ['CRAFT-REPAIR' 'MACHINE-OP-INSPCT' 'SALES' 'ARMED-FORCES' 'TECH-SUPPORT']\n",
      "insured_hobbies: ['SLEEPING' 'READING' 'BOARD-GAMES' 'BUNGIE-JUMPING' 'BASE-JUMPING']\n",
      "insured_relationship: ['HUSBAND' 'OTHER-RELATIVE' 'OWN-CHILD' 'UNMARRIED' 'WIFE']\n",
      "incident_date: ['2015-01-25' '2015-01-21' '2015-02-22' '2015-01-10' '2015-02-17']\n",
      "incident_type: ['SINGLE VEHICLE COLLISION' 'VEHICLE THEFT' 'MULTI-VEHICLE COLLISION'\n",
      " 'PARKED CAR']\n",
      "collision_type: ['SIDE COLLISION' '<NA>' 'REAR COLLISION' 'FRONT COLLISION']\n",
      "incident_severity: ['MAJOR DAMAGE' 'MINOR DAMAGE' 'TOTAL LOSS' 'TRIVIAL DAMAGE']\n",
      "authorities_contacted: ['POLICE' 'NAN' 'FIRE' 'OTHER' 'AMBULANCE']\n",
      "incident_state: ['SC' 'VA' 'NY' 'OH' 'WV']\n",
      "incident_city: ['COLUMBUS' 'RIVERWOOD' 'ARLINGTON' 'SPRINGFIELD' 'HILLSDALE']\n",
      "incident_location: ['9935 4TH DRIVE' '6608 MLK HWY' '7121 FRANCIS LANE' '6956 MAPLE DRIVE'\n",
      " '3041 3RD AVE']\n",
      "property_damage: ['YES' '<NA>' 'NO']\n",
      "police_report_available: ['YES' '<NA>' 'NO']\n",
      "auto_make: ['SAAB' 'MERCEDES' 'DODGE' 'CHEVROLET' 'ACCURA']\n",
      "auto_model: ['92X' 'E400' 'RAM' 'TAHOE' 'RSX']\n",
      "fraud_reported: ['Y' 'N']\n"
     ]
    }
   ],
   "source": [
    "# Print a message to indicate the start of the process\n",
    "print(\"Checking for leading/trailing spaces in string columns...\\n\")\n",
    "\n",
    "# Loop through all columns that are of string (object) type\n",
    "for col in df.select_dtypes(include='object'):\n",
    "    # Identify entries with leading or trailing whitespace to spot data inconsistencies\n",
    "    whitespace_mask = df[col].str.match(r'^\\s+|\\s+$', na=False)\n",
    "    count = whitespace_mask.sum()  # Count how many entries are affected\n",
    "\n",
    "    # If this column has entries with leading/trailing spaces, show samples\n",
    "    if count > 0:\n",
    "        print(f\"Column '{col}' has {count} entries with leading/trailing spaces.\")\n",
    "        print(\"   Sample problematic values:\", df.loc[whitespace_mask, col].unique()[:5])\n",
    "    else:\n",
    "        print(f\"Column '{col}' is clean (no leading/trailing spaces).\")\n",
    "\n",
    "# Inform when the cleanup is beginning\n",
    "print(\"\\nCleaning all string columns by stripping leading and trailing spaces...\\n\")\n",
    "\n",
    "# Remove unwanted whitespace from all string columns to ensure uniformity\n",
    "for col in df.select_dtypes(include='object'):\n",
    "    df[col] = df[col].str.strip()\n",
    "\n",
    "# Confirm cleanup is complete\n",
    "print(\"Cleanup complete. Sample cleaned values for each string column:\")\n",
    "\n",
    "# Show 5 sample cleaned values from each string column\n",
    "for col in df.select_dtypes(include='object'):\n",
    "    print(f\"{col}: {df[col].unique()[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439e4d5f",
   "metadata": {},
   "source": [
    "## 8. Clean the Data + Create Missing Value Flags\n",
    "\n",
    "In this step, I improve the data quality and prepare the dataset for downstream analysis and modeling by performing several critical actions:\n",
    "\n",
    "### Key Steps:\n",
    "- **Drop empty columns**: Remove any columns that are completely missing (all values are `NaN`), as they provide no useful information.\n",
    "- **Create binary missing flags**: For selected categorical columns (e.g. `collision_type`, `police_report_available`, etc.), generate `_missing_flag` columns that indicate if the original value was missing.  \n",
    "  This captures potential *informative missingness* — i.e., missing values that may correlate with fraud.\n",
    "- **Impute non-critical missing values**: Fill missing values in lower-priority columns like `police_report_available` with a default (`'NO'`) to avoid unnecessary row drops.\n",
    "- **Drop rows missing critical data**: Remove rows that are missing essential categorical features such as `incident_type` or `collision_type`, since these are needed for key analyses.\n",
    "- **Parse `policy_csl` coverage limits**: This column contains strings like `\"100/300\"` that represent min/max coverage values. I split this into two new numeric columns:\n",
    "  - `policy_csl_min`: Minimum liability coverage  \n",
    "  - `policy_csl_max`: Maximum liability coverage  \n",
    "  Robust error handling is included to catch and report parsing issues.\n",
    "\n",
    "By combining all these actions into a single cleanup step, I ensure the dataset is both **analytically reliable** and **ready for modeling** — without losing valuable patterns hidden in missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "feb693a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after dropping missing critical categories: 1000 (dropped 0 rows)\n",
      "Parsed 'policy_csl' into 'policy_csl_min' and 'policy_csl_max'.\n",
      "\n",
      "Sample of 'policy_csl' parsing results:\n",
      "  policy_csl  policy_csl_min  policy_csl_max\n",
      "0    250/500             250             500\n",
      "1    250/500             250             500\n",
      "2    100/300             100             300\n",
      "3    250/500             250             500\n",
      "4   500/1000             500            1000\n",
      "Cleaned data shape: (1000, 45)\n"
     ]
    }
   ],
   "source": [
    "# Remove any columns that contain no data at all\n",
    "df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "# Impute missing values in 'police_report_available' with 'NO' to retain rows\n",
    "df.fillna({'police_report_available': 'NO'}, inplace=True)\n",
    "\n",
    "# Store original number of rows before dropping critical missing values\n",
    "initial_rows = df.shape[0]\n",
    "\n",
    "# Create binary flags to capture missing values in selected key fields\n",
    "flag_features = ['collision_type', 'police_report_available', 'property_damage', 'authorities_contacted']\n",
    "for col in flag_features:\n",
    "    df[f'{col}_missing_flag'] = df[col].isna().astype(int)\n",
    "\n",
    "# Drop rows missing values in critical categorical features\n",
    "df.dropna(subset=['incident_type', 'collision_type'], inplace=True)\n",
    "\n",
    "# Calculate and report how many rows were dropped\n",
    "dropped_rows = initial_rows - df.shape[0]\n",
    "print(f\"Rows after dropping missing critical categories: {df.shape[0]} (dropped {dropped_rows} rows)\")\n",
    "\n",
    "# Define a helper function to parse 'policy_csl' coverage limits stored as strings like \"100/300\"\n",
    "def parse_policy_csl(csl_str):\n",
    "    try:\n",
    "        parts = csl_str.split('/')\n",
    "        return int(parts[0]), int(parts[1])\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: could not parse policy_csl value '{csl_str}': {e}\")\n",
    "        return (pd.NA, pd.NA)\n",
    "    \n",
    "# Apply parsing function to create separate numeric columns for min and max coverage limits\n",
    "if 'policy_csl' in df.columns:\n",
    "    csl_parsed = df['policy_csl'].apply(lambda x: parse_policy_csl(x) if pd.notna(x) else (pd.NA, pd.NA))\n",
    "    df[['policy_csl_min', 'policy_csl_max']] = pd.DataFrame(csl_parsed.tolist(), index=df.index)\n",
    "    print(\"Parsed 'policy_csl' into 'policy_csl_min' and 'policy_csl_max'.\")\n",
    "\n",
    "    # Display sample of parsed coverage limits alongside original values for verification\n",
    "    print(\"\\nSample of 'policy_csl' parsing results:\")\n",
    "    print(df[['policy_csl', 'policy_csl_min', 'policy_csl_max']].head())\n",
    "\n",
    "# Display new shape of cleaned dataset after these operations\n",
    "print(\"Cleaned data shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c32f32c",
   "metadata": {},
   "source": [
    "## 9. Encode & Format Columns\n",
    "\n",
    "To make the dataset machine readable, I now convert string based fields into consistent numerical and date formats:\n",
    "\n",
    "- Convert `fraud_reported` from `\"Y\"/\"N\"` to binary `1/0`\n",
    "- Format `incident_date` as a proper datetime object for time based analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c60a59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 'policy_bind_date' to datetime with 0 missing values after conversion.\n"
     ]
    }
   ],
   "source": [
    "# Convert 'fraud_reported' from categorical strings ('Y'/'N') to binary numeric values (1/0)\n",
    "df['fraud_reported'] = df['fraud_reported'].map({'Y': 1, 'N': 0})\n",
    "df['fraud_reported'] = df['fraud_reported'].fillna(0).astype(int)\n",
    "\n",
    "# Convert date columns to proper datetime objects for temporal analysis and sorting\n",
    "if 'incident_date' in df.columns:\n",
    "    df['incident_date'] = pd.to_datetime(df['incident_date'], errors='coerce')\n",
    "\n",
    "# Convert 'policy_bind_date' column to datetime format\n",
    "if 'policy_bind_date' in df.columns:\n",
    "    df['policy_bind_date'] = pd.to_datetime(df['policy_bind_date'], errors='coerce')\n",
    "    print(f\"Converted 'policy_bind_date' to datetime with {df['policy_bind_date'].isna().sum()} missing values after conversion.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dc5644",
   "metadata": {},
   "source": [
    "## 10. Create a Risk Score Feature\n",
    "\n",
    "I Introduced a simple rule based `risk_score` to help identify potentially fraudulent claims.  \n",
    "Each claim earns a \"risk point\" for meeting one of the following high-risk conditions:\n",
    "\n",
    "- **High claim amount** (greater than \\$10,000)\n",
    "- **Unusual incident time** (between 12:00 AM and 5:00 AM)\n",
    "- **No police report submitted**\n",
    "\n",
    "The final score ranges from `0 = (low risk)` to `3 = (high risk)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8745f147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "total_claim_amount",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "incident_hour_of_the_day",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "police_report_available",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "risk_score",
         "rawType": "int32",
         "type": "integer"
        }
       ],
       "ref": "413506cb-48c9-4620-a52f-8e5c76e2e564",
       "rows": [
        [
         "0",
         "71610",
         "5",
         "YES",
         "2"
        ],
        [
         "1",
         "5070",
         "8",
         "<NA>",
         "0"
        ],
        [
         "2",
         "34650",
         "7",
         "NO",
         "2"
        ],
        [
         "3",
         "63400",
         "5",
         "NO",
         "3"
        ],
        [
         "4",
         "6500",
         "20",
         "NO",
         "1"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_claim_amount</th>\n",
       "      <th>incident_hour_of_the_day</th>\n",
       "      <th>police_report_available</th>\n",
       "      <th>risk_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71610</td>\n",
       "      <td>5</td>\n",
       "      <td>YES</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5070</td>\n",
       "      <td>8</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34650</td>\n",
       "      <td>7</td>\n",
       "      <td>NO</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>63400</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6500</td>\n",
       "      <td>20</td>\n",
       "      <td>NO</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_claim_amount  incident_hour_of_the_day police_report_available  \\\n",
       "0               71610                         5                     YES   \n",
       "1                5070                         8                    <NA>   \n",
       "2               34650                         7                      NO   \n",
       "3               63400                         5                      NO   \n",
       "4                6500                        20                      NO   \n",
       "\n",
       "   risk_score  \n",
       "0           2  \n",
       "1           0  \n",
       "2           2  \n",
       "3           3  \n",
       "4           1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a basic risk score based on three binary risk conditions\n",
    "df['risk_score'] = (\n",
    "    (df['total_claim_amount'] > 10000).astype(int) +                    # 1 point if claim amount exceeds $10,000\n",
    "    (df['incident_hour_of_the_day'].between(0, 5)).astype(int) +        # 1 point if incident occurred between midnight and 5AM\n",
    "    (df['police_report_available'].str.upper() == 'NO').astype(int)     # 1 point if no police report was filed\n",
    ")\n",
    "\n",
    "# Preview relevant fields used in the risk score calculation\n",
    "df[['total_claim_amount', 'incident_hour_of_the_day', 'police_report_available', 'risk_score']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce12e90f",
   "metadata": {},
   "source": [
    "## 11. Final Sanity Check Before Exporting\n",
    "\n",
    "Before saving and exporting the cleaned dataset, I perform a final sanity check to verify:\n",
    "\n",
    "- The overall size of the dataset (number of rows and columns)\n",
    "- The total number of fraud cases (`fraud_reported = 1`)\n",
    "- The total number of non-fraud cases (`fraud_reported = 0`)\n",
    "\n",
    "This helps ensure the data encoding and filtering steps have worked correctly and that the dataset is ready for downstream processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65d47356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset shape: (1000, 46)\n",
      "Number of fraud cases: 247\n",
      "Number of non-fraud cases: 753\n"
     ]
    }
   ],
   "source": [
    "# Final sanity check before saving/exporting\n",
    "print(\"Final dataset shape:\", df.shape)\n",
    "print(\"Number of fraud cases:\", df['fraud_reported'].sum())\n",
    "print(\"Number of non-fraud cases:\", (df['fraud_reported'] == 0).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77336f30",
   "metadata": {},
   "source": [
    "## 12. Save the Cleaned Dataset\n",
    "\n",
    "Now that I've cleaned and preprocessed the data, I'll save it to a **CSV** file.\n",
    "\n",
    "Saving a copy ensures:\n",
    "- You don’t need to re-clean the raw dataset every time.\n",
    "- The data can be reused for **EDA**, modeling, or dashboarding.\n",
    "- It creates an auditable snapshot of your cleaned version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74bad53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved to:\n",
      "C:\\Users\\Cloud\\OneDrive\\Desktop\\Fraud_Analytics_Project\\data\\cleaned\\cleaned_insurance_claims.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the cleaned DataFrame to a CSV file\n",
    "df.to_csv(cleaned_file, index=False)\n",
    "\n",
    "# Confirm the saved file location\n",
    "print(f\"Cleaned dataset saved to:\\n{cleaned_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf631ff",
   "metadata": {},
   "source": [
    "## 13. Load Cleaned Data into PostgreSQL\n",
    "\n",
    "After saving the cleaned dataset locally, I load it into a **PostgreSQL** table for further analysis and querying.\n",
    "\n",
    "This step enables:\n",
    "- Centralized access to the data via **SQL** tools (e.g., `pgAdmin`, `Tableau`)\n",
    "- Easier integration with reporting dashboards\n",
    "- Scalable querying of clean data without relying on raw **CSVs**\n",
    "\n",
    "The table will be named: `insurance_claims`.\n",
    "If it already exists, it will be **replaced**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5903fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded into PostgreSQL table: 'insurance_claims'\n"
     ]
    }
   ],
   "source": [
    "# Load the cleaned DataFrame into a PostgreSQL table database (my table name is 'insurance_claims')\n",
    "try:\n",
    "    df.to_sql(\"insurance_claims\", engine, if_exists=\"replace\", index=False)\n",
    "    print(\"Data loaded into PostgreSQL table: 'insurance_claims'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data into PostgreSQL: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026011dd",
   "metadata": {},
   "source": [
    "## 14. Query Fraud vs Non-Fraud Counts\n",
    "\n",
    "To validate that the data was successfully inserted and to get a quick summary of fraud distribution,  \n",
    "I will run a simple **SQL** query to count how many claims were reported as fraud (`1`) vs not fraud (`0`).\n",
    "\n",
    "This helps:\n",
    "- Verify correct encoding of the `fraud_reported` field\n",
    "- Understand basic class balance before modeling\n",
    "- Confirm that data loaded into **PostgreSQL** is accessible and queryable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2c6f2e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "fraud_reported",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "16939aeb-ab8c-4b89-85e7-c6eb7ce69f7a",
       "rows": [
        [
         "0",
         "0",
         "753"
        ],
        [
         "1",
         "1",
         "247"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fraud_reported</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fraud_reported  count\n",
       "0               0    753\n",
       "1               1    247"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SQL query to count the number of claims by fraud label\n",
    "query = \"\"\"\n",
    "SELECT fraud_reported, COUNT(*) as count\n",
    "FROM insurance_claims\n",
    "GROUP BY fraud_reported;\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and display results\n",
    "fraud_summary = pd.read_sql(query, engine)\n",
    "fraud_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6ba096",
   "metadata": {},
   "source": [
    "# ETL Summary\n",
    "\n",
    "- Raw insurance claim data was cleaned, standardized, and validated  \n",
    "- Placeholder values and inconsistencies were handled  \n",
    "- Missing value flags were created for downstream analysis  \n",
    "- The cleaned dataset was exported locally as a CSV  \n",
    "- Data was loaded into a PostgreSQL database (`insurance_claims` table via pgAdmin4)  \n",
    "- A fraud distribution validation query confirmed successful ingestion\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Perform Exploratory Data Analysis in [`eda.ipynb`](./eda.ipynb) to understand fraud patterns  \n",
    "- Engineer predictive features in [`feature_engineering.ipynb`](./feature_engineering.ipynb)  \n",
    "- Train machine learning models in [`model_training.ipynb`](./model_training.ipynb)  \n",
    "- Visualize insights and fraud risks using Tableau in `reporting_dashboard.ipynb`\n",
    "\n",
    "---\n",
    "\n",
    "This ETL process forms the foundation of the fraud analytics pipeline by ensuring the dataset is high quality, structured, and ready for in depth analysis and machine learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
