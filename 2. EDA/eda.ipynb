{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9b15b5b",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) for Insurance Fraud Dataset\n",
    "\n",
    "This notebook explores the cleaned **Insurance Claims** Dataset to understand:\n",
    "- **Data Structure** and **Summary Statistics**\n",
    "- `Fraud` vs `non-fraud` **Distribution** and **Imbalance**\n",
    "- **Distribution** of key **Numeric Features**\n",
    "- **Relationships** between **Features** and **Fraud**\n",
    "- **Categorical** **Feature Analysis**\n",
    "- **Time-based** **Trends** (if available)\n",
    "- **Outlier** **Detection**\n",
    "\n",
    "I use `pandas` for **Data Manipulation** and `seaborn`/`matplotlib` for **Visualization**.\n",
    "\n",
    "## Table of Contents\n",
    "1. Load Data and Basic Inspection  \n",
    "2. Data Overview and Summary Statistics  \n",
    "3. Fraud Class Distribution  \n",
    "4. Convert Fraud Labels for Numeric Analysis  \n",
    "5. Numeric Feature Exploration  \n",
    "6. Categorical Feature Exploration  \n",
    "7. Time-Based Trends \n",
    "8. EDA Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d64739f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing essential Python libraries\n",
    "import pandas as pd                                 # For data manipulation\n",
    "import seaborn as sns                               # For statistical visualizations\n",
    "import numpy as np                                  # For numerical operations\n",
    "import matplotlib.pyplot as plt                     # For basic plotting\n",
    "import os                                           # For file path handling\n",
    "from IPython.display import display, Markdown       # For displaying Markdown and DataFrames in Jupyter\n",
    "\n",
    "# Set Seaborn's visual style for cleaner plots\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Enable inline plotting for Jupyter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c5cb85",
   "metadata": {},
   "source": [
    "## 1. Load Data and Basic Inspection\n",
    "\n",
    "In this step, I load the cleaned insurance claims dataset from a CSV file saved from the ETL process into a pandas DataFrame. I also inspect the shape (rows, columns) to understand dataset size and display the first few rows to get an initial feel for the dataâ€™s structure and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a36734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define project directory and cleaned data path\n",
    "project_dir = r\"C:\\Users\\Cloud\\OneDrive\\Desktop\\Fraud_Analytics_Project\"\n",
    "cleaned_file = os.path.join(project_dir, \"data\", \"cleaned\", \"cleaned_insurance_claims.csv\")\n",
    "\n",
    "# Load cleaned dataset with date parsing for incident_date and policy_bind_date\n",
    "try:\n",
    "    # Attempt to load the cleaned dataset, parsing dates in specified columns\n",
    "    df = pd.read_csv(cleaned_file, parse_dates=['incident_date', 'policy_bind_date'])\n",
    "    # This will inform me that the file was loaded successfully with the exact path.\n",
    "    display(Markdown(f\"**Loaded data from:** `{cleaned_file}`\"))\n",
    "except FileNotFoundError:\n",
    "    # This line catches the specific error when the file is not found.\n",
    "    display(Markdown(f\"**Error:** File not found at `{cleaned_file}`. Please check the file path.\"))\n",
    "    # Raise error or exit\n",
    "    raise\n",
    "except Exception as e:\n",
    "    # Catches any other unexpected exceptions during file loading.\n",
    "    display(Markdown(f\"**Error loading data:** {e}\"))\n",
    "    # Raises the error again after displaying the message.\n",
    "    raise\n",
    "\n",
    "# Display dataframe shape\n",
    "display(Markdown(\"**Dataframe shape:**\"))\n",
    "display(df.shape)\n",
    "\n",
    "# Show first 5 rows for initial inspection\n",
    "display(Markdown(\"**First 5 rows:**\"))\n",
    "display(df.head())\n",
    "\n",
    "# Show dataframe info for datatypes and non-null counts\n",
    "display(Markdown(\"**Dataframe info:**\"))\n",
    "df.info()\n",
    "\n",
    "# Confirm 'fraud_reported' column is numeric and show value counts\n",
    "display(Markdown(\"**Fraud reported value counts:**\"))\n",
    "display(df['fraud_reported'].value_counts())\n",
    "\n",
    "# Optionally create a human-readable label for plotting convenience\n",
    "df['fraud_label'] = df['fraud_reported'].map({0: 'No', 1: 'Yes'})\n",
    "\n",
    "# Display sample of fraud_reported and fraud_label columns\n",
    "display(Markdown(\"**Sample fraud_reported and fraud_label:**\"))\n",
    "display(df[['fraud_reported', 'fraud_label']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a543352c",
   "metadata": {},
   "outputs": [],
   "source": [
    "[col for col in df.columns if 'missing' in col.lower()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301b5846",
   "metadata": {},
   "source": [
    "## 2. Data Overview and Summary Statistics\n",
    "\n",
    "Here, I review the dataset's structure using `info()` to check data types and missing values. I then display summary statistics (mean, median, quartiles, etc.) for numeric columns, helping me understand feature distributions and spot anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfcb2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many missing values in each column, sorted descending\n",
    "missing_counts = df.isna().sum().sort_values(ascending=False)\n",
    "\n",
    "# Calculate percentage of missing values relative to total rows, rounded to 2 decimals\n",
    "missing_percent = (missing_counts / len(df) * 100).round(2)\n",
    "\n",
    "# Combine missing counts and percentages into a DataFrame for easier viewing\n",
    "missing_summary = pd.DataFrame({'Missing Count': missing_counts, 'Missing %': missing_percent})\n",
    "\n",
    "# Display missing value summary table\n",
    "display(missing_summary)\n",
    "\n",
    "# List all columns that have any missing values\n",
    "cols_with_missing = missing_counts[missing_counts > 0].index.tolist()\n",
    "display(Markdown(f\"**Columns with missing values:** {cols_with_missing}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24870b9a",
   "metadata": {},
   "source": [
    "## 3. Fraud Class Distribution\n",
    "\n",
    "In this section, I explore the distribution of fraudulent vs non-fraudulent claims.\n",
    "This is important because fraud datasets are often **imbalanced**, which can affect model performance. I show both the **counts and percentages** of each class, and visualize the imbalance using an annotated bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eca6efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of fraud (1) and non-fraud (0) cases\n",
    "fraud_counts = df['fraud_reported'].value_counts().sort_index()\n",
    "fraud_percentages = (fraud_counts / fraud_counts.sum() * 100).round(2)\n",
    "\n",
    "# Plot countplot using numeric labels and annotated percentages\n",
    "plt.figure(figsize=(6, 4))\n",
    "ax = sns.countplot(x='fraud_reported', data=df, palette=['#66b3ff', '#ff6666'])\n",
    "\n",
    "# Annotate each bar with count and percentage\n",
    "for i, count in enumerate(fraud_counts):\n",
    "    percent = fraud_percentages[i]\n",
    "    ax.text(i, count + 100, f\"{count} ({percent}%)\", ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.title('Fraud vs Non-Fraud Claim Counts with Percentages')\n",
    "plt.xlabel('Fraud Reported (0=No, 1=Yes)')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Display fraud counts and imbalance ratio\n",
    "display(Markdown(\"### Fraud Class Breakdown\"))\n",
    "display(fraud_counts)\n",
    "imbalance_ratio = fraud_counts.max() / fraud_counts.min()\n",
    "display(Markdown(f\"**Class imbalance ratio (majority/minority):** {imbalance_ratio:.2f}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c2bbfd",
   "metadata": {},
   "source": [
    "## 4. Convert Fraud Labels for Numeric Analysis\n",
    "\n",
    "The `fraud_reported` column is already numeric (`0` for non-fraud, `1` for fraud).  \n",
    "To ensure compatibility with modeling workflows and visual consistency, I duplicate it to a new column `fraud_numeric`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d163bea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate the 'fraud_reported' column to a new 'fraud_numeric' column for clarity in modeling workflows\n",
    "df['fraud_numeric'] = df['fraud_reported']\n",
    "\n",
    "# Display the first 5 rows of all three columns to confirm the transformation\n",
    "display(df[['fraud_reported', 'fraud_numeric', 'fraud_label']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49258dd4",
   "metadata": {},
   "source": [
    "## 5. Numeric Feature Exploration\n",
    "\n",
    "In this section, I explore key **numerical features** to better understand:\n",
    "\n",
    "- Their **distributions** across all claims (normality, skewness, shape)\n",
    "- Their **relationship to fraud** using boxplots by fraud label\n",
    "- The presence of **outliers** that may signal unusual or fraudulent activity\n",
    "\n",
    "These insights are essential for:\n",
    "- Identifying features that may help differentiate fraud vs. non-fraud claims\n",
    "- Detecting values that require **transformation or scaling** before modeling\n",
    "- Understanding **data quality issues**, like impossible values (e.g., invalid hours)\n",
    "\n",
    "### Key Features Analyzed:\n",
    "- `total_claim_amount` â€“ amount claimed by policyholder  \n",
    "- `incident_hour_of_the_day` â€“ time of day when the incident occurred  \n",
    "- `risk_score` â€“ engineered fraud risk signal created during ETL\n",
    "\n",
    "---\n",
    "> NOTE:\n",
    "> High-value claims and incidents occurring late at night are often considered red flags in insurance fraud.  \n",
    "> This numeric feature exploration helps us **quantify how such risk related variables behave**,  \n",
    "> and whether they demonstrate **distinct patterns between fraudulent and non-fraudulent claims**.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56a47cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of important numeric features to analyze\n",
    "numeric_features = ['total_claim_amount', 'incident_hour_of_the_day', 'risk_score']\n",
    "\n",
    "# Check for impossible or invalid hour values (should be between 0 and 23)\n",
    "invalid_hours = df[(df['incident_hour_of_the_day'] < 0) | (df['incident_hour_of_the_day'] > 23)]\n",
    "display(Markdown(f\"**Invalid 'incident_hour_of_the_day' entries detected:** {len(invalid_hours)}\"))\n",
    "\n",
    "# If invalid entries exist, display a sample for debugging\n",
    "if len(invalid_hours) > 0:\n",
    "    display(Markdown(\"### Sample invalid 'incident_hour_of_the_day' rows\"))\n",
    "    display(invalid_hours.head())\n",
    "\n",
    "# Generate descriptive statistics (mean, std, min/max, quartiles) for numeric features\n",
    "display(Markdown(\"### Descriptive Statistics\"))\n",
    "display(df[numeric_features].describe().T)\n",
    "\n",
    "# Visualize distribution and fraud-based separation for each numeric feature\n",
    "for feature in numeric_features:\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    # Plot histogram with KDE to visualize overall distribution\n",
    "    sns.histplot(df[feature], bins=30, kde=True, color='skyblue', ax=axs[0])\n",
    "    axs[0].set_title(f'Distribution of {feature}')\n",
    "    axs[0].set_xlabel(feature)\n",
    "    axs[0].set_ylabel('Number of Claims')\n",
    "    axs[0].grid(True)\n",
    "\n",
    "    # Plot boxplot split by fraud class to detect shifts or differences in distributions\n",
    "    sns.boxplot(x='fraud_numeric', y=feature, data=df, palette=['#66b3ff', '#ff6666'], ax=axs[1])\n",
    "    axs[1].set_title(f'{feature} by Fraud Status')\n",
    "    axs[1].set_xlabel('Fraud Reported (0 = No, 1 = Yes)')\n",
    "    axs[1].set_ylabel(feature)\n",
    "\n",
    "    # Set overall plot title\n",
    "    plt.suptitle(f'{feature}: Distribution & Fraud Comparison', fontsize=14)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])  # Reserve space for title\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "# Correlation Heatmap for Numeric Features\n",
    "display(Markdown(\"### Correlation Heatmap of Numeric Features\"))\n",
    "\n",
    "# Compute correlation matrix for numeric features\n",
    "corr_matrix = df[numeric_features].corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title('Correlation Heatmap of Numeric Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Display top 5 largest claims with relevant details for context\n",
    "top_5_claims = df.nlargest(5, 'total_claim_amount')\n",
    "\n",
    "# Select columns of interest for context, adjust as per your dataset\n",
    "cols_to_show = ['claim_number', 'total_claim_amount', 'fraud_reported', 'incident_type', 'collision_type']\n",
    "\n",
    "# Check if all columns exist, otherwise adjust cols_to_show\n",
    "existing_cols = [col for col in cols_to_show if col in df.columns]\n",
    "\n",
    "# Display the top 5 claims with selected columns\n",
    "display(Markdown(\"### Top 5 Largest Claims with Details\"))\n",
    "display(top_5_claims[existing_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7b3373",
   "metadata": {},
   "source": [
    "## 6. Categorical Feature Analysis: Fraud Relationships\n",
    "\n",
    "This section investigates how key **categorical features** relate to fraud, using both visual and statistical analysis.\n",
    "\n",
    "### Goals:\n",
    "- Explore how categories (e.g., collision type) distribute across fraud and non-fraud cases\n",
    "- Identify categories with **higher fraud rates**\n",
    "- Visualize **fraud proportions** using stacked bar charts\n",
    "- Examine whether **missing categorical values** signal higher fraud likelihood\n",
    "\n",
    "These features are domain-relevant and often cited in insurance fraud studies:\n",
    "- `incident_type`: Type of incident may influence likelihood of fraudulent reporting.\n",
    "- `collision_type`: Specific collisions may be staged or prone to fraud.\n",
    "- `police_report_available`: Lack of police involvement may indicate suspicious behavior.\n",
    "\n",
    "Key components of this section:\n",
    "- **Countplots** showing category distribution across fraud vs. non-fraud\n",
    "- **Fraud rate tables** sorted by likelihood across categories\n",
    "- **Stacked bar charts** for visualizing proportion of fraud in each category\n",
    "- **Missing value flags** analyzed for hidden fraud signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4291f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define relevant categorical features for fraud analysis\n",
    "categorical_features = ['incident_type', 'collision_type', 'police_report_available']\n",
    "\n",
    "# 1. Plot frequency of each category split by fraud label\n",
    "plt.figure(figsize=(18, 5))\n",
    "for i, feature in enumerate(categorical_features, 1):\n",
    "    plt.subplot(1, 3, i)\n",
    "    sns.countplot(x=feature, hue='fraud_reported', data=df, palette=['#66b3ff', '#ff6666'])\n",
    "    plt.title(f'{feature} by Fraud Reported')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# 2. Show fraud rate by category (sorted high to low)\n",
    "for feature in categorical_features:\n",
    "    summary = df.groupby(feature).agg(\n",
    "        count=(feature, 'size'),\n",
    "        fraud_rate=('fraud_numeric', 'mean')\n",
    "    ).sort_values('fraud_rate', ascending=False)\n",
    "    \n",
    "    display(Markdown(f\"### Fraud Rate Summary for `{feature}`\"))\n",
    "    display(Markdown(\"_Sorted descending by fraud rate to highlight categories with highest fraud risk._\"))\n",
    "    display(summary)\n",
    "\n",
    "# 3. Visualize fraud vs. non-fraud proportions per category\n",
    "for feature in categorical_features:\n",
    "    crosstab = pd.crosstab(df[feature], df['fraud_reported'], normalize='index')\n",
    "    crosstab.plot(kind='bar', stacked=True, figsize=(8, 4), colormap='Paired')\n",
    "    plt.title(f'Fraud Proportions by {feature}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Proportion of Claims')\n",
    "    plt.legend(title='Fraud Reported')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "# 4. Check how missing value flags relate to fraud rates\n",
    "display(Markdown(\"### Impact of Missing Values on Fraud Rate\"))\n",
    "\n",
    "# List of expected missing value indicator columns\n",
    "missing_flags = ['collision_type_missing_flag', 'police_report_available_missing_flag']\n",
    "\n",
    "for flag_col in missing_flags:\n",
    "    if flag_col in df.columns:\n",
    "        # Group by missing flag: count cases and calculate fraud rate\n",
    "        summary = df.groupby(flag_col).agg(\n",
    "            count=('fraud_reported', 'size'),\n",
    "            fraud_rate=('fraud_reported', 'mean')\n",
    "        ).sort_index(ascending=False)  # Show True (missing) first\n",
    "\n",
    "        display(Markdown(f\"#### Fraud Rate by `{flag_col}`\"))\n",
    "        display(summary)\n",
    "    else:\n",
    "        display(Markdown(f\"`{flag_col}` not found in dataset. Make sure it was created during ETL.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cb7dc7",
   "metadata": {},
   "source": [
    "## 7. Time Based Trends\n",
    "\n",
    "Analyzing fraud trends over time may reveal seasonal spikes, evolving fraud tactics, or reporting delays. This insight supports operational decisions, such as increasing fraud monitoring during certain periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40ca621",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'incident_date' in df.columns:\n",
    "    # Convert incident_date to datetime (handle errors by coercing invalid formats to NaT)\n",
    "    df['incident_date'] = pd.to_datetime(df['incident_date'], errors='coerce')\n",
    "    \n",
    "    # Count how many rows will be dropped due to invalid date\n",
    "    num_invalid_dates = df['incident_date'].isna().sum()\n",
    "    display(Markdown(f\"**Rows with invalid 'incident_date' removed:** {num_invalid_dates}\"))\n",
    "\n",
    "    # Drop rows where incident_date could not be parsed\n",
    "    time_df = df.dropna(subset=['incident_date'])\n",
    "    \n",
    "    # Group by date and fraud status to count claims per day\n",
    "    fraud_over_time = time_df.groupby(['incident_date', 'fraud_reported']).size().unstack(fill_value=0)\n",
    "    \n",
    "    # Add a note about the aggregation granularity and noise\n",
    "    display(Markdown(\n",
    "        \"**Note:** The plot shows daily claim counts, which can be noisy. \" \n",
    "        \"Consider aggregating by week or month or applying smoothing for clearer trends.\"\n",
    "    ))\n",
    "    \n",
    "    # Plot fraud and non-fraud claims over time\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    fraud_over_time.plot(ax=plt.gca())\n",
    "    plt.title('Fraud vs Non-Fraud Claims Over Time')\n",
    "    plt.xlabel('Incident Date')\n",
    "    plt.ylabel('Number of Claims')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "else:\n",
    "    display(Markdown(\"**No 'incident_date' column available to analyze time trends.**\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c8d092",
   "metadata": {},
   "source": [
    "# EDA Summary\n",
    "\n",
    "- Checked Data Quality and Missing Values  \n",
    "- Explored Fraud Class Imbalance  \n",
    "- Analyzed Numeric and Categorical Feature Distributions  \n",
    "- Examined Feature Relationships with Fraud  \n",
    "- Investigated Missing Data Impact  \n",
    "- Visualized Time Trends and Outliers\n",
    "\n",
    "## Next Steps\n",
    "- Feature Engineering and Selection  \n",
    "- Train/Test Split and Model Development  \n",
    "- Model Evaluation and Tuning\n",
    "\n",
    "---\n",
    "\n",
    "## Transition to Modeling\n",
    "\n",
    "The fraud prediction pipeline continues in [`model_training.ipynb`](./model_training.ipynb), where we:\n",
    "- Engineer new fraud-predictive features  \n",
    "- Split data into train/test sets  \n",
    "- Train baseline and advanced models  \n",
    "- Evaluate fraud detection performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
