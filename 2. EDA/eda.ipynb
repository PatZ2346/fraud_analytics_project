{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9b15b5b",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) for Insurance Fraud Dataset\n",
    "\n",
    "This notebook explores the cleaned **Insurance Claims** Dataset to understand:\n",
    "- **Data Structure** and **Summary Statistics**\n",
    "- `Fraud` vs `non-fraud` **Distribution** and **Imbalance**\n",
    "- **Distribution** of key **Numeric Features**\n",
    "- **Relationships** between **Features** and **Fraud**\n",
    "- **Categorical** **Feature Analysis**\n",
    "- **Time-based** **Trends** (if available)\n",
    "- **Outlier** **Detection**\n",
    "\n",
    "I use `pandas` for **Data Manipulation** and `seaborn`/`matplotlib` for **Visualization**.\n",
    "\n",
    "## Table of Contents\n",
    "1. Load Data and Basic Inspection  \n",
    "2. Data Overview and Summary Statistics  \n",
    "3. Fraud Class Distribution  \n",
    "4. Convert Fraud Labels for Numeric Analysis  \n",
    "5. Numeric Feature Exploration  \n",
    "6. Categorical Feature Exploration  \n",
    "7. Time-Based Trends \n",
    "8. EDA Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d64739f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing essential Python libraries\n",
    "import pandas as pd                                 # For data manipulation\n",
    "import seaborn as sns                               # For statistical visualizations\n",
    "import matplotlib.pyplot as plt                     # For basic plotting\n",
    "import os                                           # For file path handling\n",
    "from IPython.display import display, Markdown       # For displaying Markdown and DataFrames in Jupyter\n",
    "\n",
    "# Set Seaborn's visual style for cleaner plots\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Enable inline plotting for Jupyter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c5cb85",
   "metadata": {},
   "source": [
    "## 1. Load Data and Basic Inspection\n",
    "\n",
    "This step I load the cleaned insurance claims dataset from a CSV file saved from the ETL process into a pandas DataFrame. I also inspect the shape (rows, columns) to understand dataset size and display the first few rows to get an initial feel for the dataâ€™s structure and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a36734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the project directory path (update as per your environment)\n",
    "project_dir = r\"C:\\Users\\Cloud\\OneDrive\\Desktop\\Fraud_Analytics_Project\"\n",
    "\n",
    "# Construct full path to the cleaned CSV file within the project folder\n",
    "cleaned_file = os.path.join(project_dir, \"data\", \"cleaned\", \"cleaned_insurance_claims.csv\")\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(cleaned_file)\n",
    "\n",
    "# Display the shape of the dataframe (rows, columns)\n",
    "display(Markdown(\"**Dataframe shape:**\"))\n",
    "display(df.shape)\n",
    "\n",
    "# Show the first 5 rows to get a sense of the data\n",
    "display(Markdown(\"**First 5 rows:**\"))\n",
    "display(df.head())\n",
    "\n",
    "# Provide info about data types and non-null counts per column\n",
    "display(Markdown(\"**Dataframe info:**\"))\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301b5846",
   "metadata": {},
   "source": [
    "## 2. Data Overview and Summary Statistics\n",
    "\n",
    "Here, I review the dataset's structure using `info()` to check data types and missing values. I then display summary statistics (mean, median, quartiles, etc.) for numeric columns, helping me understand feature distributions and spot anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfcb2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many missing values in each column, sorted descending\n",
    "missing_counts = df.isna().sum().sort_values(ascending=False)\n",
    "\n",
    "# Calculate percentage of missing values relative to total rows, rounded to 2 decimals\n",
    "missing_percent = (missing_counts / len(df) * 100).round(2)\n",
    "\n",
    "# Combine missing counts and percentages into a DataFrame for easier viewing\n",
    "missing_summary = pd.DataFrame({'Missing Count': missing_counts, 'Missing %': missing_percent})\n",
    "\n",
    "# Display missing value summary table\n",
    "display(missing_summary)\n",
    "\n",
    "# List all columns that have any missing values\n",
    "cols_with_missing = missing_counts[missing_counts > 0].index.tolist()\n",
    "display(Markdown(f\"**Columns with missing values:** {cols_with_missing}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24870b9a",
   "metadata": {},
   "source": [
    "## 3. Fraud Class Distribution\n",
    "\n",
    "I examine how many claims are labeled as fraudulent vs non-fraudulent, including the percentage imbalance. This is crucial because many fraud datasets are imbalanced, which can affect model training and evaluation. I then visualize this imbalance with a count plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eca6efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of fraud ('Y') and non-fraud ('N') cases\n",
    "fraud_counts = df['fraud_reported'].value_counts()\n",
    "\n",
    "# Display raw counts for each fraud class\n",
    "display(fraud_counts)\n",
    "\n",
    "# Calculate percentage of fraud cases (if 'Y' present in data)\n",
    "fraud_percentage = fraud_counts.get('Y', 0) / fraud_counts.sum() * 100 if 'Y' in fraud_counts else 0\n",
    "\n",
    "# Display fraud percentage as markdown for clarity\n",
    "display(Markdown(f\"**Percentage of fraud cases:** {fraud_percentage:.2f}%\"))\n",
    "\n",
    "# Plot a bar chart showing counts of fraud vs non-fraud claims\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x='fraud_reported', data=df, palette=['#66b3ff', '#ff6666'])\n",
    "plt.title('Fraud vs Non-Fraud Claim Counts')\n",
    "plt.xlabel('Fraud Reported (Y/N)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display class imbalance ratio (majority class count divided by minority class count)\n",
    "imbalance_ratio = fraud_counts.max() / fraud_counts.min()\n",
    "display(Markdown(f\"**Class imbalance ratio (majority/minority):** {imbalance_ratio:.2f}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c2bbfd",
   "metadata": {},
   "source": [
    "## 4. Convert Fraud Labels for Numeric Analysis\n",
    "\n",
    "I stored fraud labels as text, here (`\"Y\"`, `\"N\"`) are converted into a numeric format (`1` for `fraud`, `0` for `non-fraud`). This numeric encoding simplifies later statistical calculations and visualizations involving fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d163bea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map string labels 'Y'/'Yes' to 1 (fraud), and 'N'/'No' to 0 (non-fraud)\n",
    "df['fraud_numeric'] = df['fraud_reported'].map({'Y': 1, 'N': 0, 'Yes': 1, 'No': 0})\n",
    "\n",
    "# Show sample of the original and converted fraud columns to confirm mapping\n",
    "display(Markdown(\"**Sample conversion:**\"))\n",
    "display(df[['fraud_reported', 'fraud_numeric']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49258dd4",
   "metadata": {},
   "source": [
    "## 5. Numeric Feature Exploration\n",
    "\n",
    "In this section, I analyze key numeric features to understand their:\n",
    "\n",
    "- **Overall distribution** (using histograms and KDE)\n",
    "- **Relationship with fraud** (via boxplots split by fraud class)\n",
    "- **Presence of extreme values/outliers** (especially in claim amounts)\n",
    "\n",
    "This helps identify skewed variables, potential transformation needs, and features that may differentiate fraud from non-fraud claims.\n",
    "\n",
    "### Features Analyzed:\n",
    "- `total_claim_amount`  \n",
    "- `incident_hour_of_the_day`  \n",
    "- `risk_score` *(engineered during ETL)*\n",
    "\n",
    "Key components:\n",
    "- **Histogram & Boxplot per feature**\n",
    "- **Descriptive statistics**\n",
    "- **Top 5 highest claim amounts**\n",
    "- **Invalid hour check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56a47cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric Feature Exploration (Distribution + Fraud Comparison + Outliers)\n",
    "numeric_features = ['total_claim_amount', 'incident_hour_of_the_day', 'risk_score']\n",
    "\n",
    "# Check for invalid values in incident_hour_of_the_day\n",
    "invalid_hours = df[(df['incident_hour_of_the_day'] < 0) | (df['incident_hour_of_the_day'] > 23)]\n",
    "display(Markdown(f\"**Invalid 'incident_hour_of_the_day' entries:** {len(invalid_hours)}\"))\n",
    "\n",
    "# Summary Statistics\n",
    "display(Markdown(\"### Descriptive Statistics\"))\n",
    "display(df[numeric_features].describe().T)\n",
    "\n",
    "# Combined Distribution (Histogram + Boxplot)\n",
    "for feature in numeric_features:\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    # Histogram\n",
    "    sns.histplot(df[feature], bins=30, kde=True, color='skyblue', ax=axs[0])\n",
    "    axs[0].set_title(f'Distribution of {feature}')\n",
    "    axs[0].set_xlabel(feature)\n",
    "    axs[0].set_ylabel('Count')\n",
    "\n",
    "    # Boxplot by Fraud\n",
    "    sns.boxplot(x='fraud_numeric', y=feature, data=df, palette=['#66b3ff', '#ff6666'], ax=axs[1])\n",
    "    axs[1].set_title(f'{feature} by Fraud Reported')\n",
    "    axs[1].set_xlabel('Fraud Reported (0=No, 1=Yes)')\n",
    "    axs[1].set_ylabel(feature)\n",
    "\n",
    "    plt.suptitle(f'{feature}: Distribution & Fraud Comparison', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Outlier Detection: Top 5 largest total claims\n",
    "display(Markdown(\"### Top 5 Largest 'total_claim_amount' Claims\"))\n",
    "top_5_claims = df['total_claim_amount'].sort_values(ascending=False).head()\n",
    "display(top_5_claims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7b3373",
   "metadata": {},
   "source": [
    "## 6.Categorical Feature Exploration\n",
    "\n",
    "This section investigates how key **categorical features** relate to fraud, using both visual and statistical analysis.\n",
    "\n",
    "### Goals:\n",
    "- Explore how categories (e.g., collision type) distribute across fraud and non-fraud cases\n",
    "- Identify categories with **higher fraud rates**\n",
    "- Visualize **fraud proportions** using stacked bar charts\n",
    "- Examine whether **missing categorical values** signal higher fraud likelihood\n",
    "\n",
    "### Features Analyzed:\n",
    "- `incident_type`\n",
    "- `collision_type`\n",
    "- `police_report_available`\n",
    "\n",
    "Key components:\n",
    "- **Countplots split by fraud**\n",
    "- **Fraud rate summary tables** (sorted by fraud likelihood)\n",
    "- **Stacked bar charts** showing proportional fraud\n",
    "- **Fraud rate comparison between missing vs. non-missing groups**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4291f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical Feature Exploration (Distribution + Fraud Rates + Missingness)\n",
    "display(Markdown(\"##Categorical Feature Exploration\"))\n",
    "\n",
    "categorical_features = ['incident_type', 'collision_type', 'police_report_available']\n",
    "\n",
    "# Countplots Split by Fraud\n",
    "plt.figure(figsize=(18, 5))\n",
    "for i, feature in enumerate(categorical_features, 1):\n",
    "    plt.subplot(1, 3, i)\n",
    "    sns.countplot(x=feature, hue='fraud_reported', data=df, palette=['#66b3ff', '#ff6666'])\n",
    "    plt.title(f'{feature} by Fraud Reported')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Fraud Rate Summary Tables\n",
    "for feature in categorical_features:\n",
    "    summary = df.groupby(feature).agg(\n",
    "        count=(feature, 'size'),\n",
    "        fraud_rate=('fraud_numeric', 'mean')\n",
    "    ).sort_values('fraud_rate', ascending=False)\n",
    "\n",
    "    display(Markdown(f\"###Fraud Rate Summary for `{feature}`\"))\n",
    "    display(summary)\n",
    "\n",
    "# Stacked Bar Charts (Fraud Proportions)\n",
    "for feature in categorical_features:\n",
    "    crosstab = pd.crosstab(df[feature], df['fraud_reported'], normalize='index')\n",
    "    crosstab.plot(kind='bar', stacked=True, figsize=(8, 4), colormap='coolwarm')\n",
    "    plt.title(f'Fraud Proportions by {feature}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Proportion of Claims')\n",
    "    plt.legend(title='Fraud Reported')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Missingness Impact on Fraud Rate\n",
    "display(Markdown(\"###Impact of Missing Values on Fraud Rate\"))\n",
    "\n",
    "# Create flags for missing values\n",
    "df['collision_type_missing'] = df['collision_type'].isna()\n",
    "df['police_report_missing'] = df['police_report_available'].isna()\n",
    "\n",
    "# Fraud rate by missingness\n",
    "missing_features = ['collision_type_missing', 'police_report_missing']\n",
    "for missing_col in missing_features:\n",
    "    summary = df.groupby(missing_col)['fraud_numeric'].mean().to_frame('fraud_rate')\n",
    "    label = missing_col.replace('_missing', '')\n",
    "    display(Markdown(f\"#### Fraud Rate: `{label}` Missing vs Not Missing\"))\n",
    "    display(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cb7dc7",
   "metadata": {},
   "source": [
    "## 7. Time Based Trends\n",
    "\n",
    "If `incident_date` data exist, this section will analyze fraud trends over time. This can reveal patterns, seasonality, or sudden spikes in fraud occurrences, useful for fraud detection and resource planning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40ca621",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'incident_date' in df.columns:\n",
    "    # Convert incident_date to datetime (handle errors by coercing)\n",
    "    df['incident_date'] = pd.to_datetime(df['incident_date'], errors='coerce')\n",
    "    \n",
    "    # Drop rows where incident_date could not be parsed\n",
    "    time_df = df.dropna(subset=['incident_date'])\n",
    "    \n",
    "    # Group by date and fraud status to count claims per day\n",
    "    fraud_over_time = time_df.groupby(['incident_date', 'fraud_reported']).size().unstack(fill_value=0)\n",
    "    \n",
    "    # Plot fraud and non-fraud claims over time\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    fraud_over_time.plot(ax=plt.gca())\n",
    "    plt.title('Fraud vs Non-Fraud Claims Over Time')\n",
    "    plt.xlabel('Incident Date')\n",
    "    plt.ylabel('Number of Claims')\n",
    "    plt.show()\n",
    "else:\n",
    "    display(Markdown(\"**No 'incident_date' column available to analyze time trends.**\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c8d092",
   "metadata": {},
   "source": [
    "# EDA Summary\n",
    "\n",
    "- Checked Data Quality and Missing Values  \n",
    "- Explored Fraud Class Imbalance  \n",
    "- Analyzed Numeric and Categorical Feature Distributions  \n",
    "- Examined Feature Relationships with Fraud  \n",
    "- Investigated Missing Data Impact  \n",
    "- Visualized Time Trends and Outliers\n",
    "\n",
    "## Next Steps\n",
    "- Feature Engineering and Selection  \n",
    "- Train/Test Split and Model Development  \n",
    "- Model Evaluation and Tuning\n",
    "\n",
    "---\n",
    "\n",
    "## Transition to Modeling\n",
    "\n",
    "The fraud prediction pipeline continues in [`model_training.ipynb`](./model_training.ipynb), where we:\n",
    "- Engineer new fraud-predictive features  \n",
    "- Split data into train/test sets  \n",
    "- Train baseline and advanced models  \n",
    "- Evaluate fraud detection performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
